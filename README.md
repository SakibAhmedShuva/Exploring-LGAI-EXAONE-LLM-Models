# ğŸš€ Exploring LGAI-EXAONE LLM Models for NER

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org)

This repository contains Jupyter notebooks and resources for exploring and evaluating different versions and quantization methods of the **LGAI-EXAONE Large Language Models**, primarily focusing on Named Entity Recognition (NER) tasks with structured JSON output.

---

## ğŸ“‹ Overview

### Primary Goals
1. **Load and run** various LGAI-EXAONE models (base, GGUF, AWQ quantized)
2. **Evaluate performance** on sample NER tasks
3. **Compare inference speed** and output quality across different model formats
4. **Provide practical guidance** for setting up and using these models

> ğŸ’¡ **Note**: All models are instructed to produce JSON-only responses for easier parsing and integration.

---

## ğŸ¤– Models Explored

### EXAONE Models
| Model | Type | Description |
|-------|------|-------------|
| `LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct` | Base | The base instruction-tuned model |
| `LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-GGUF` | GGUF | Quantized versions for CPU/GPU inference with `llama-cpp-python` |
| `LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-AWQ` | AWQ | Quantized version for optimized inference with Hugging Face Transformers |

### Comparison Models (Granite)
- `bartowski/granite-3.1-2b-instruct-GGUF`
- `ibm-granite/granite-3.3-2b-instruct-GGUF`

---

## ğŸ““ Notebooks

### 1. `lgai-exaone-gguf.ipynb`
- **ğŸ¯ Purpose**: Demonstrates loading and running GGUF quantized models (EXAONE and Granite variants)
- **ğŸ“š Key Libraries**: `llama-cpp-python`
- **ğŸ” Focus**: Measures inference time for NER task on various GGUF quantization levels
- **âš™ï¸ Quantization Levels**: Q8_0, IQ4_XS, Q4_0, Q2_K, Q4_K_S

### 2. `lgai-exaone_AWQ.ipynb`
- **ğŸ¯ Purpose**: Shows how to load and run the AWQ (Activation-aware Weight Quantization) version
- **ğŸ“š Key Libraries**: `transformers`, `autoawq`, `torch`
- **ğŸ” Focus**: Measures inference time for NER task on AWQ quantized EXAONE model

### 3. `lgai-exaone.ipynb`
- **ğŸ¯ Purpose**: Demonstrates the base `LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct` model
- **ğŸ“š Key Libraries**: `transformers`, `torch`
- **ğŸ” Focus**: Baseline performance and output for NER task with full-precision model

> âš ï¸ **Note**: The current content of `lgai-exaone.ipynb` may include AWQ model loading. This should be clarified/cleaned if it's meant for base model comparison.

---

## ğŸ› ï¸ Setup and Installation

### Prerequisites
- **Python**: 3.9+
- **Package Manager**: `pip`
- **Hardware**: NVIDIA GPU with CUDA drivers (highly recommended)
- **Build Tools**: `cmake`, C++ compiler (if compiling `llama-cpp-python` from source)

### Installation Steps

#### 1. Clone the Repository
```bash
git clone https://github.com/SakibAhmedShuva/Exploring-LGAI-EXAONE-LLM-Models.git
cd Exploring-LGAI-EXAONE-LLM-Models
```

#### 2. Create Virtual Environment (Recommended)
```bash
python -m venv venv

# Linux/Mac
source venv/bin/activate

# Windows
venv\Scripts\activate
```

#### 3. Install Dependencies

**For GGUF Models** (`lgai-exaone-gguf.ipynb`):
```bash
pip install -r requirements.txt

# For CUDA support (if needed):
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir -U
```

**For AWQ and Base Transformers** (`lgai-exaone_AWQ.ipynb`, `lgai-exaone.ipynb`):
```bash
pip install -r requirements.txt
```

> ğŸ’¡ **Tip**: Consider separate requirements files (`requirements_gguf.txt`, `requirements_awq.txt`) if dependencies conflict.

---

## ğŸš€ How to Run

### Quick Start
1. âœ… Complete the **Setup and Installation** steps
2. âœ… Activate your virtual environment
3. âœ… Launch Jupyter:
   ```bash
   jupyter lab
   # or
   jupyter notebook
   ```
4. âœ… Open desired notebook (`.ipynb` file)
5. âœ… Run cells sequentially

### Important Notes
- ğŸ“¦ First code cell usually installs necessary packages
- ğŸ”„ Subsequent cells define prompts, load models, and perform inference
- â³ Model downloads can take time and require significant disk space

---

## ğŸ“ Example Task: Named Entity Recognition

All notebooks use a structured approach for NER tasks:

### System Prompt Structure
```markdown
You are a JSON-only response system. Follow these rules absolutely:
ONLY output valid, parseable JSON
...
```

### Test Instruction Example
```markdown
extract NER:
California
DRIVER LICENSE
dl 11234568
...
```

---

## ğŸ“Š Key Observations

### GGUF Models Performance
| Quantization | Quality | Speed | Size | Use Case |
|--------------|---------|-------|------|----------|
| **Q8_0** | â­â­â­â­ | â­â­â­ | â­â­ | **Recommended balance** |
| **Q4_0** | â­â­â­ | â­â­â­â­ | â­â­â­â­ | Fast inference |
| **Q2_K** | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | Maximum speed |
| **BF16** | â­â­â­â­â­ | â­â­ | â­ | Highest quality |

### AWQ Models
- âœ… **Significant speedup** over base model
- âœ… **Minimal quality loss** for NER tasks
- âš ï¸ Requires `autoawq` and compatible `transformers` versions

### Inference Performance
- ğŸš€ **AWQ models on GPU**: Generally very fast
- âš–ï¸ **GGUF models**: Flexible for CPU/GPU with varying performance by quantization
- ğŸ’» **Hardware dependency**: Performance varies significantly with available hardware

---

## ğŸ“„ License

This repository is licensed under the **[MIT License](LICENSE)**. 

> âš ï¸ **Important**: Model weights are subject to their own licenses as specified by their respective Hugging Face repositories.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ Support

If you encounter any issues or have questions, please open an issue in this repository.

---

<div align="center">

**â­ Star this repository if you find it helpful! â­**

Made with â¤ï¸ for the ML community

</div>
